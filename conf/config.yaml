# Default config for PsychoBench. Override any key from the CLI, e.g.:
#   python run_psychobench.py model=gpt-4 questionnaire=BFI,EPQ-R
# Each run creates a timestamped output directory under results/.
hydra:
  run:
    dir: results/${now:%Y-%m-%d}/${now:%H-%M-%S}

# Model to test. Use any LiteLLM model string, e.g.:
#   - openai/gpt-3.5-turbo, gpt-4, gpt-4o
#   - anthropic/claude-3-opus-20240229, claude-3-5-sonnet
#   - gemini/gemini-pro, google/gemini-1.5-flash
#   - azure/gpt-4 (requires api_base), openrouter/anthropic/claude-3-opus
model: gpt-3.5-turbo

# Comma-separated list of questionnaires, or ALL.
questionnaire: ALL

# Number of question orders: 0 = original only; n > 0 = original + n permutations.
shuffle_count: 9

# Number of runs per order.
test_count: 1

# Name of this run (used for result filenames). Null => use model name.
name_exp: null

# Significance level for hypothesis testing (human vs LLM means).
significance_level: 0.01

# Pipeline mode: auto (full), generation, testing, or analysis.
mode: auto

# API keys per provider (from env or override via config/CLI).
# Set in .env or export: OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY.
openai_api_key: ${oc.env:OPENAI_API_KEY,""}
anthropic_api_key: ${oc.env:ANTHROPIC_API_KEY,""}
google_api_key: ${oc.env:GEMINI_API_KEY,""}

# Optional: Custom API base URL (e.g. Azure, OpenAI-compatible proxies).
api_base: ""
