# Default config for PsychoBench. Override any key from the CLI, e.g.:
#   python run_psychobench.py model=gpt-4 questionnaire=BFI,EPQ-R
# Each run creates a timestamped output directory under results/.
hydra:
  run:
    dir: results/${now:%Y-%m-%d}/${now:%H-%M-%S}

# Model to test. Must start with a provider prefix (see allowed_providers), e.g.:
#   - openai/gpt-3.5-turbo, openai/gpt-4, openai/gpt-4o
#   - anthropic/claude-3-opus-20240229, anthropic/claude-3-5-sonnet
#   - gemini/gemini-pro, gemini/gemini-1.5-flash
#   - ollama/llama2, ollama/llama3.2 (local; no API key needed)
model: ollama/deepseek-r1:latest

# Allowed LiteLLM provider prefixes. Model must start with one of these (e.g. openai/..., anthropic/..., gemini/..., ollama/...).
allowed_providers:
  - gemini
  - anthropic
  - openai
  - ollama

# Comma-separated list of questionnaires, or ALL.
questionnaire: BFI

# Number of question orders: 0 = original only; n > 0 = original + n permutations.
shuffle_count: 1

# Number of runs per order.
test_count: 1

# Name of this run (used for result filenames). Null => use model name.
name_exp: null

# Significance level for hypothesis testing (human vs LLM means).
significance_level: 0.01

# Pipeline mode: auto (full), generation, testing, or analysis.
mode: auto

# Sampling temperature for LLM calls (0 = deterministic; 1 = more random).
# Some models (e.g. Gemini 3) recommend temperature=1.0; override per run if needed.
temperature: 0.01

# LiteLLM reads API keys from environment variables (e.g. OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY).

# Optional: Custom API base URL (e.g. Azure, OpenAI-compatible proxies).
api_base: ""
