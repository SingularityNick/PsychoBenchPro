# Default config for PsychoBench. Override any key from the CLI, e.g.:
#   python run_psychobench.py model=gpt-4 questionnaire=BFI,EPQ-R
# Each run creates a timestamped output directory under results/.
defaults:
  - override hydra/launcher: joblib

hydra:
  run:
    dir: results/${now:%Y-%m-%d}/${now:%H-%M-%S}

# Model to test. Must start with a provider prefix (see allowed_providers), e.g.:
#   - openai/gpt-3.5-turbo, openai/gpt-4, openai/gpt-4o
#   - anthropic/claude-3-opus-20240229, anthropic/claude-3-5-sonnet
#   - gemini/gemini-pro, gemini/gemini-1.5-flash
#   - ollama/llama2, ollama/llama3.2 (local; no API key needed)
model: ollama/deepseek-r1:latest

# Allowed LiteLLM provider prefixes. Model must start with one of these (e.g. openai/..., anthropic/..., gemini/..., ollama/...).
allowed_providers:
  - gemini
  - anthropic
  - openai
  - ollama

# Comma-separated list of questionnaires, or ALL.
questionnaire: BFI

# Number of question orders: 0 = original only; n > 0 = original + n permutations.
shuffle_count: 1

# Number of runs per order.
test_count: 1

# Name of this run (used for result filenames). Null => use model name.
name_exp: null

# Significance level for hypothesis testing (human vs LLM means).
significance_level: 0.01

# Pipeline mode: auto (full), generation, testing, or analysis.
mode: auto

# Sampling temperature for LLM calls (0 = deterministic; 1 = more random).
# Some models (e.g. Gemini 3) recommend temperature=1.0; override per run if needed.
temperature: 1.0

# Use structured output (JSON mode with a Pydantic schema) for more reliable response parsing.
# When true, the LLM is asked to return a JSON object matching a schema instead of free text.
# Requires a model that supports structured output / JSON mode (most modern models do).
# When false, falls back to the legacy text-based parsing (extract last digit per line).
use_structured_output: true

# Batch size for splitting questionnaire questions into chunks for LLM calls.
# Default 30: questions are sent in batches of 30 per API call.
# When null: no batching â€” the full questionnaire is sent as a single batch regardless of size.
batch_size: 30

# LiteLLM reads API keys from environment variables (e.g. OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY).

# Optional: Custom API base URL (e.g. Azure, OpenAI-compatible proxies).
api_base: ""
