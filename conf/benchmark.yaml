# Benchmark config: multirun over the listed models.
# Run with: uv run python run_psychobench.py --config-name benchmark
# Pricing: see scripts/PRICING_REFERENCE.md (official docs + how cost is calculated).
defaults:
  - config
  - _self_

# Use specific datestamps/versions whenever the provider documents them (e.g. 20250929 for
# Sonnet 4.5, gpt-4o-2024-11-20). This improves reproducibility when providers update aliases.
models:
  # Direct: Anthropic, Google (Gemini), OpenAI
  - anthropic/claude-sonnet-4-20250514
  - anthropic/claude-sonnet-4-5-20250929
  - anthropic/claude-opus-4-6
  - anthropic/claude-sonnet-4-6
  - gemini/gemini-2.5-pro
  - gemini/gemini-3-pro-preview
  - gemini/gemini-3.1-pro-preview
  - openai/gpt-3.5-turbo-0125
  - openai/gpt-4
  - openai/gpt-4.1-2025-04-14
  - openai/gpt-4o-2024-11-20
  - openai/gpt-5-2025-08-07
  - openai/gpt-5.2-2025-12-11
  - openai/gpt-5.2-pro-2025-12-11
  - openai/text-davinci-003
  # OpenRouter: DeepSeek, Moonshot (Kimi), Meta Llama, Qwen (no Gemini Flash: distillation of Pro)
  - openrouter/deepseek/deepseek-v3
  - openrouter/deepseek/deepseek-v3.2
  - openrouter/moonshotai/kimi-k2
  - openrouter/meta-llama/llama-3.1-405b-instruct
  - openrouter/meta-llama/llama-3.1-70b-instruct
  - openrouter/meta-llama/llama-4-maverick-17b-128e-instruct
  - openrouter/meta-llama/llama-2-13b
  - openrouter/meta-llama/llama-2-7b
  - openrouter/qwen/qwen2.5-72b-instruct
  - openrouter/qwen/qwen3-235b-a22b-instruct
  - openrouter/qwen/qwen3-32b
  - openrouter/qwen/qwen3-coder-480b-a35b-instruct



# No batching (context windows should be large enough)
batch_size: null

# 2 orders. Original + 1 permutation.
shuffle_count: 9

hydra:
  mode: MULTIRUN
  sweeper:
    params:
      model: ${join:${models}}
