# Benchmark config: multirun over the listed models.
# Run with: uv run python run_psychobench.py --config-name benchmark
defaults:
  - config
  - _self_

models:
  - gemini/gemini-3-pro-preview
  - gemini/gemini-3-flash-preview
  - openai/gpt-5.2-2025-12-11
  - openai/gpt-5-mini-2025-08-07
  # - openai/gpt-5-nano-2025-08-07
  - openai/gpt-5.2-pro-2025-12-11
  # - openai/gpt-5-2025-08-07
  # - openai/gpt-4.1-2025-04-14
  - anthropic/claude-opus-4-6
  # - anthropic/claude-opus-4-5-20251101
  # - anthropic/claude-opus-4-1-20250805
  # - anthropic/claude-opus-4-20250514
  - anthropic/claude-sonnet-4-6
  # - anthropic/claude-sonnet-4-5-20250929
  # - anthropic/claude-sonnet-4-20250514
  # - anthropic/claude-haiku-4-5-20251001
  # - anthropic/claude-3-haiku-20240307


# No batching (context windows should be large enough)
batch_size: null

# 2 orders. Original + 1 permutation.
shuffle_count: 9

hydra:
  mode: MULTIRUN
  sweeper:
    params:
      model: ${join:${models}}
